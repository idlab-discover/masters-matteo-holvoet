==== START logs for container kube-controller-manager of pod kube-system/kube-controller-manager-response-34-control-plane ====
I0103 23:21:22.042196       1 serving.go:386] Generated self-signed cert in-memory
I0103 23:21:22.479547       1 controllermanager.go:197] "Starting" version="v1.31.0"
I0103 23:21:22.479584       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0103 23:21:22.481165       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I0103 23:21:22.481171       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I0103 23:21:22.482382       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0103 23:21:22.482478       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0103 23:21:22.482555       1 leaderelection.go:254] attempting to acquire leader lease kube-system/kube-controller-manager...
E0103 23:21:24.472426       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
I0103 23:21:28.834179       1 leaderelection.go:268] successfully acquired lease kube-system/kube-controller-manager
I0103 23:21:28.834297       1 event.go:389] "Event occurred" object="kube-system/kube-controller-manager" fieldPath="" kind="Lease" apiVersion="coordination.k8s.io/v1" type="Normal" reason="LeaderElection" message="response-34-control-plane_26c4b3f4-7f86-4844-a5b4-09af4a699dd0 became leader"
I0103 23:21:28.848372       1 controllermanager.go:797] "Started controller" controller="serviceaccount-token-controller"
I0103 23:21:28.848390       1 shared_informer.go:313] Waiting for caches to sync for tokens
I0103 23:21:28.920391       1 controllermanager.go:797] "Started controller" controller="deployment-controller"
I0103 23:21:28.920820       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I0103 23:21:28.920882       1 shared_informer.go:313] Waiting for caches to sync for deployment
I0103 23:21:28.949290       1 shared_informer.go:320] Caches are synced for tokens
I0103 23:21:28.971832       1 controllermanager.go:797] "Started controller" controller="horizontal-pod-autoscaler-controller"
I0103 23:21:28.971944       1 horizontal.go:201] "Starting HPA controller" logger="horizontal-pod-autoscaler-controller"
I0103 23:21:28.971957       1 shared_informer.go:313] Waiting for caches to sync for HPA
I0103 23:21:29.046328       1 controllermanager.go:797] "Started controller" controller="clusterrole-aggregation-controller"
I0103 23:21:29.046354       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="service-cidr-controller" requiredFeatureGates=["MultiCIDRServiceAllocator"]
I0103 23:21:29.046377       1 clusterroleaggregation_controller.go:194] "Starting ClusterRoleAggregator controller" logger="clusterrole-aggregation-controller"
I0103 23:21:29.046387       1 shared_informer.go:313] Waiting for caches to sync for ClusterRoleAggregator
I0103 23:21:29.073484       1 controllermanager.go:797] "Started controller" controller="job-controller"
I0103 23:21:29.073607       1 job_controller.go:226] "Starting job controller" logger="job-controller"
I0103 23:21:29.073633       1 shared_informer.go:313] Waiting for caches to sync for job
I0103 23:21:29.115131       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-approving-controller"
I0103 23:21:29.115236       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
I0103 23:21:29.115262       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrapproving
E0103 23:21:29.130036       1 core.go:274] "Failed to start cloud node lifecycle controller" err="no cloud provider provided" logger="cloud-node-lifecycle-controller"
I0103 23:21:29.130079       1 controllermanager.go:775] "Warning: skipping controller" controller="cloud-node-lifecycle-controller"
I0103 23:21:29.130088       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
I0103 23:21:29.177159       1 controllermanager.go:797] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0103 23:21:29.177277       1 shared_informer.go:313] Waiting for caches to sync for validatingadmissionpolicy-status
I0103 23:21:29.191085       1 controllermanager.go:797] "Started controller" controller="endpointslice-mirroring-controller"
I0103 23:21:29.191258       1 endpointslicemirroring_controller.go:227] "Starting EndpointSliceMirroring controller" logger="endpointslice-mirroring-controller"
I0103 23:21:29.191282       1 shared_informer.go:313] Waiting for caches to sync for endpoint_slice_mirroring
I0103 23:21:29.192640       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0103 23:21:29.192807       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0103 23:21:29.206996       1 controllermanager.go:797] "Started controller" controller="persistentvolume-attach-detach-controller"
I0103 23:21:29.207240       1 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I0103 23:21:29.207259       1 shared_informer.go:313] Waiting for caches to sync for attach detach
I0103 23:21:29.220245       1 controllermanager.go:797] "Started controller" controller="ephemeral-volume-controller"
I0103 23:21:29.220323       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0103 23:21:29.220347       1 shared_informer.go:313] Waiting for caches to sync for ephemeral
I0103 23:21:29.298236       1 controllermanager.go:797] "Started controller" controller="bootstrap-signer-controller"
I0103 23:21:29.298287       1 shared_informer.go:313] Waiting for caches to sync for bootstrap_signer
I0103 23:21:29.497247       1 controllermanager.go:797] "Started controller" controller="token-cleaner-controller"
I0103 23:21:29.497376       1 tokencleaner.go:117] "Starting token cleaner controller" logger="token-cleaner-controller"
I0103 23:21:29.497401       1 shared_informer.go:313] Waiting for caches to sync for token_cleaner
I0103 23:21:29.497410       1 shared_informer.go:320] Caches are synced for token_cleaner
I0103 23:21:29.664076       1 controllermanager.go:797] "Started controller" controller="persistentvolume-expander-controller"
I0103 23:21:29.664118       1 controllermanager.go:775] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0103 23:21:29.664170       1 expand_controller.go:328] "Starting expand controller" logger="persistentvolume-expander-controller"
I0103 23:21:29.664211       1 shared_informer.go:313] Waiting for caches to sync for expand
I0103 23:21:29.756726       1 controllermanager.go:797] "Started controller" controller="endpointslice-controller"
I0103 23:21:29.756786       1 endpointslice_controller.go:281] "Starting endpoint slice controller" logger="endpointslice-controller"
I0103 23:21:29.756794       1 shared_informer.go:313] Waiting for caches to sync for endpoint_slice
I0103 23:21:29.987569       1 garbagecollector.go:146] "Starting controller" logger="garbage-collector-controller" controller="garbagecollector"
I0103 23:21:29.987597       1 controllermanager.go:797] "Started controller" controller="garbage-collector-controller"
I0103 23:21:29.987608       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0103 23:21:29.987634       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I0103 23:21:30.283351       1 controllermanager.go:797] "Started controller" controller="replicaset-controller"
I0103 23:21:30.283421       1 replica_set.go:217] "Starting controller" logger="replicaset-controller" name="replicaset"
I0103 23:21:30.283430       1 shared_informer.go:313] Waiting for caches to sync for ReplicaSet
I0103 23:21:30.399377       1 controllermanager.go:797] "Started controller" controller="statefulset-controller"
I0103 23:21:30.399420       1 stateful_set.go:166] "Starting stateful set controller" logger="statefulset-controller"
I0103 23:21:30.399432       1 shared_informer.go:313] Waiting for caches to sync for stateful set
I0103 23:21:30.437357       1 node_lifecycle_controller.go:430] "Controller will reconcile labels" logger="node-lifecycle-controller"
I0103 23:21:30.437403       1 controllermanager.go:797] "Started controller" controller="node-lifecycle-controller"
I0103 23:21:30.437446       1 node_lifecycle_controller.go:464] "Sending events to api server" logger="node-lifecycle-controller"
I0103 23:21:30.437480       1 node_lifecycle_controller.go:475] "Starting node controller" logger="node-lifecycle-controller"
I0103 23:21:30.437490       1 shared_informer.go:313] Waiting for caches to sync for taint
I0103 23:21:30.606458       1 controllermanager.go:797] "Started controller" controller="root-ca-certificate-publisher-controller"
I0103 23:21:30.606529       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0103 23:21:30.606537       1 shared_informer.go:313] Waiting for caches to sync for crt configmap
I0103 23:21:30.840368       1 controllermanager.go:797] "Started controller" controller="namespace-controller"
I0103 23:21:30.840406       1 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
I0103 23:21:30.840412       1 shared_informer.go:313] Waiting for caches to sync for namespace
I0103 23:21:31.038060       1 controllermanager.go:797] "Started controller" controller="disruption-controller"
I0103 23:21:31.038128       1 disruption.go:452] "Sending events to api server." logger="disruption-controller"
I0103 23:21:31.038157       1 disruption.go:463] "Starting disruption controller" logger="disruption-controller"
I0103 23:21:31.038165       1 shared_informer.go:313] Waiting for caches to sync for disruption
I0103 23:21:31.087892       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-signing-controller"
I0103 23:21:31.087980       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-legacy-unknown"
I0103 23:21:31.087995       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0103 23:21:31.088030       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-serving"
I0103 23:21:31.088036       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0103 23:21:31.088057       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-client"
I0103 23:21:31.088082       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0103 23:21:31.088107       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kube-apiserver-client"
I0103 23:21:31.088230       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0103 23:21:31.088179       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I0103 23:21:31.088192       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I0103 23:21:31.088202       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I0103 23:21:31.088170       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I0103 23:21:31.241118       1 range_allocator.go:112] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses" logger="node-ipam-controller"
I0103 23:21:31.241162       1 controllermanager.go:797] "Started controller" controller="node-ipam-controller"
I0103 23:21:31.241217       1 node_ipam_controller.go:141] "Starting ipam controller" logger="node-ipam-controller"
I0103 23:21:31.241240       1 shared_informer.go:313] Waiting for caches to sync for node
I0103 23:21:31.395539       1 controllermanager.go:797] "Started controller" controller="persistentvolume-protection-controller"
I0103 23:21:31.395581       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0103 23:21:31.395591       1 shared_informer.go:313] Waiting for caches to sync for PV protection
I0103 23:21:31.542678       1 controllermanager.go:797] "Started controller" controller="ttl-after-finished-controller"
I0103 23:21:31.542724       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0103 23:21:31.542731       1 shared_informer.go:313] Waiting for caches to sync for TTL after finished
I0103 23:21:31.691133       1 controllermanager.go:797] "Started controller" controller="replicationcontroller-controller"
I0103 23:21:31.691306       1 replica_set.go:217] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I0103 23:21:31.691319       1 shared_informer.go:313] Waiting for caches to sync for ReplicationController
I0103 23:21:31.850155       1 controllermanager.go:797] "Started controller" controller="pod-garbage-collector-controller"
I0103 23:21:31.850235       1 gc_controller.go:99] "Starting GC controller" logger="pod-garbage-collector-controller"
I0103 23:21:31.850321       1 shared_informer.go:313] Waiting for caches to sync for GC
I0103 23:21:31.990822       1 controllermanager.go:797] "Started controller" controller="cronjob-controller"
I0103 23:21:31.990976       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I0103 23:21:31.990998       1 shared_informer.go:313] Waiting for caches to sync for cronjob
I0103 23:21:32.036979       1 controllermanager.go:797] "Started controller" controller="taint-eviction-controller"
I0103 23:21:32.037026       1 taint_eviction.go:281] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
I0103 23:21:32.037035       1 taint_eviction.go:287] "Sending events to api server" logger="taint-eviction-controller"
I0103 23:21:32.037044       1 shared_informer.go:313] Waiting for caches to sync for taint-eviction-controller
I0103 23:21:32.190680       1 controllermanager.go:797] "Started controller" controller="endpoints-controller"
I0103 23:21:32.190982       1 endpoints_controller.go:182] "Starting endpoint controller" logger="endpoints-controller"
I0103 23:21:32.191005       1 shared_informer.go:313] Waiting for caches to sync for endpoint
I0103 23:21:32.340709       1 controllermanager.go:797] "Started controller" controller="serviceaccount-controller"
I0103 23:21:32.340758       1 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
I0103 23:21:32.340765       1 shared_informer.go:313] Waiting for caches to sync for service account
I0103 23:21:32.489924       1 controllermanager.go:797] "Started controller" controller="ttl-controller"
I0103 23:21:32.489939       1 core.go:298] "Warning: configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes." logger="node-route-controller"
I0103 23:21:32.489943       1 controllermanager.go:775] "Warning: skipping controller" controller="node-route-controller"
I0103 23:21:32.489972       1 ttl_controller.go:127] "Starting TTL controller" logger="ttl-controller"
I0103 23:21:32.489978       1 shared_informer.go:313] Waiting for caches to sync for TTL
I0103 23:21:32.639963       1 controllermanager.go:797] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0103 23:21:32.639988       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
I0103 23:21:32.640020       1 pvc_protection_controller.go:105] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I0103 23:21:32.640026       1 shared_informer.go:313] Waiting for caches to sync for PVC protection
I0103 23:21:32.790940       1 controllermanager.go:797] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I0103 23:21:32.790992       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I0103 23:21:32.791001       1 shared_informer.go:313] Waiting for caches to sync for legacy-service-account-token-cleaner
I0103 23:21:33.090252       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="daemonsets.apps"
I0103 23:21:33.090286       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="controllerrevisions.apps"
I0103 23:21:33.090294       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="cronjobs.batch"
I0103 23:21:33.090310       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ingresses.networking.k8s.io"
W0103 23:21:33.090321       1 shared_informer.go:597] resyncPeriod 14h58m40.770837577s is smaller than resyncCheckPeriod 15h13m19.858057074s and the informer has already started. Changing it to 15h13m19.858057074s
I0103 23:21:33.090355       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="statefulsets.apps"
I0103 23:21:33.090399       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="serviceaccounts"
I0103 23:21:33.090429       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podtemplates"
I0103 23:21:33.090437       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="limitranges"
I0103 23:21:33.090443       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="deployments.apps"
I0103 23:21:33.090451       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="replicasets.apps"
I0103 23:21:33.090456       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpoints"
I0103 23:21:33.090469       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="jobs.batch"
I0103 23:21:33.090494       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="poddisruptionbudgets.policy"
I0103 23:21:33.090506       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="csistoragecapacities.storage.k8s.io"
I0103 23:21:33.090532       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="horizontalpodautoscalers.autoscaling"
I0103 23:21:33.090559       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="networkpolicies.networking.k8s.io"
I0103 23:21:33.090567       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="leases.coordination.k8s.io"
I0103 23:21:33.090581       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="rolebindings.rbac.authorization.k8s.io"
I0103 23:21:33.090607       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="roles.rbac.authorization.k8s.io"
I0103 23:21:33.090632       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpointslices.discovery.k8s.io"
I0103 23:21:33.090665       1 controllermanager.go:797] "Started controller" controller="resourcequota-controller"
I0103 23:21:33.090681       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I0103 23:21:33.090695       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0103 23:21:33.090726       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I0103 23:21:33.242259       1 controllermanager.go:797] "Started controller" controller="daemonset-controller"
I0103 23:21:33.242322       1 daemon_controller.go:294] "Starting daemon sets controller" logger="daemonset-controller"
I0103 23:21:33.242329       1 shared_informer.go:313] Waiting for caches to sync for daemon sets
E0103 23:21:33.389886       1 core.go:105] "Failed to start service controller" err="WARNING: no cloud provider provided, services of type LoadBalancer will fail" logger="service-lb-controller"
I0103 23:21:33.389911       1 controllermanager.go:775] "Warning: skipping controller" controller="service-lb-controller"
I0103 23:21:33.541465       1 controllermanager.go:797] "Started controller" controller="persistentvolume-binder-controller"
I0103 23:21:33.541637       1 pv_controller_base.go:308] "Starting persistent volume controller" logger="persistentvolume-binder-controller"
I0103 23:21:33.541665       1 shared_informer.go:313] Waiting for caches to sync for persistent volume
I0103 23:21:33.544514       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0103 23:21:33.547445       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"response-34-control-plane\" does not exist"
I0103 23:21:33.549013       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0103 23:21:33.550512       1 shared_informer.go:320] Caches are synced for GC
I0103 23:21:33.557805       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0103 23:21:33.565091       1 shared_informer.go:320] Caches are synced for expand
I0103 23:21:33.572353       1 shared_informer.go:320] Caches are synced for HPA
I0103 23:21:33.574673       1 shared_informer.go:320] Caches are synced for job
I0103 23:21:33.577873       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0103 23:21:33.584135       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0103 23:21:33.588900       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0103 23:21:33.588921       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0103 23:21:33.588930       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0103 23:21:33.588939       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0103 23:21:33.590083       1 shared_informer.go:320] Caches are synced for TTL
I0103 23:21:33.591277       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0103 23:21:33.591300       1 shared_informer.go:320] Caches are synced for cronjob
I0103 23:21:33.591327       1 shared_informer.go:320] Caches are synced for endpoint
I0103 23:21:33.591379       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0103 23:21:33.596158       1 shared_informer.go:320] Caches are synced for PV protection
I0103 23:21:33.598343       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0103 23:21:33.599547       1 shared_informer.go:320] Caches are synced for stateful set
I0103 23:21:33.606767       1 shared_informer.go:320] Caches are synced for crt configmap
I0103 23:21:33.616142       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0103 23:21:33.621432       1 shared_informer.go:320] Caches are synced for ephemeral
I0103 23:21:33.621449       1 shared_informer.go:320] Caches are synced for deployment
I0103 23:21:33.637779       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0103 23:21:33.637793       1 shared_informer.go:320] Caches are synced for taint
I0103 23:21:33.637869       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0103 23:21:33.637933       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="response-34-control-plane"
I0103 23:21:33.637996       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0103 23:21:33.641084       1 shared_informer.go:320] Caches are synced for service account
I0103 23:21:33.641107       1 shared_informer.go:320] Caches are synced for namespace
I0103 23:21:33.641180       1 shared_informer.go:320] Caches are synced for PVC protection
I0103 23:21:33.642237       1 shared_informer.go:320] Caches are synced for node
I0103 23:21:33.642280       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0103 23:21:33.642292       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0103 23:21:33.642298       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0103 23:21:33.642302       1 shared_informer.go:320] Caches are synced for cidrallocator
I0103 23:21:33.642424       1 shared_informer.go:320] Caches are synced for daemon sets
I0103 23:21:33.643607       1 shared_informer.go:320] Caches are synced for TTL after finished
I0103 23:21:33.647170       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0103 23:21:33.672779       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="response-34-control-plane" podCIDRs=["10.244.0.0/24"]
I0103 23:21:33.672808       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-control-plane"
I0103 23:21:33.672838       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-control-plane"
I0103 23:21:33.692225       1 shared_informer.go:320] Caches are synced for ReplicationController
I0103 23:21:33.738746       1 shared_informer.go:320] Caches are synced for disruption
I0103 23:21:33.745104       1 shared_informer.go:320] Caches are synced for resource quota
I0103 23:21:33.790794       1 shared_informer.go:320] Caches are synced for resource quota
I0103 23:21:33.808234       1 shared_informer.go:320] Caches are synced for attach detach
I0103 23:21:33.842795       1 shared_informer.go:320] Caches are synced for persistent volume
I0103 23:21:34.158458       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-control-plane"
I0103 23:21:34.249711       1 shared_informer.go:320] Caches are synced for garbage collector
I0103 23:21:34.287710       1 shared_informer.go:320] Caches are synced for garbage collector
I0103 23:21:34.287746       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0103 23:21:34.702817       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="151.285269ms"
I0103 23:21:34.710574       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="159.019123ms"
I0103 23:21:34.759959       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="57.092475ms"
I0103 23:21:34.760030       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="22.101µs"
I0103 23:21:34.765336       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="54.709397ms"
I0103 23:21:34.765429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="32.001µs"
I0103 23:21:37.676547       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"response-34-worker\" does not exist"
I0103 23:21:37.873024       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"response-34-worker2\" does not exist"
I0103 23:21:37.971453       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="response-34-worker" podCIDRs=["10.244.1.0/24"]
I0103 23:21:37.971516       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:21:37.971556       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:21:38.481083       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="response-34-worker2" podCIDRs=["10.244.2.0/24"]
I0103 23:21:38.481109       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:21:38.525376       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:21:38.638424       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="response-34-worker2"
I0103 23:21:38.638431       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="response-34-worker"
E0103 23:21:38.957744       1 range_allocator.go:427] "Failed to update node PodCIDR after multiple attempts" err="failed to patch node CIDR: Node \"response-34-worker2\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.3.0/24\", \"10.244.2.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="response-34-worker2" podCIDRs=["10.244.3.0/24"]
E0103 23:21:38.957797       1 range_allocator.go:433] "CIDR assignment for node failed. Releasing allocated CIDR" err="failed to patch node CIDR: Node \"response-34-worker2\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.3.0/24\", \"10.244.2.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="response-34-worker2"
E0103 23:21:38.957844       1 range_allocator.go:246] "Unhandled Error" err="error syncing 'response-34-worker2': failed to patch node CIDR: Node \"response-34-worker2\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.3.0/24\", \"10.244.2.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid], requeuing" logger="UnhandledError"
I0103 23:21:38.957866       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:21:38.963708       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:21:39.665871       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:21:42.379908       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="wordpress-namespace/wordpress-79d969d897" duration="781.665335ms"
I0103 23:21:42.665010       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="wordpress-namespace/wordpress-79d969d897" duration="285.016129ms"
I0103 23:21:42.665186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="wordpress-namespace/wordpress-79d969d897" duration="26.301µs"
I0103 23:21:48.623708       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:21:52.129908       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-control-plane"
I0103 23:21:52.160404       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-control-plane"
I0103 23:21:52.199468       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="43.102µs"
I0103 23:21:52.199683       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="19.901µs"
I0103 23:21:52.234053       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="34.901µs"
I0103 23:21:52.346459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="48.002µs"
I0103 23:21:52.364095       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="55.301µs"
I0103 23:21:52.394751       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="52.202µs"
I0103 23:21:53.640493       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0103 23:21:59.365006       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="response-34-worker"
I0103 23:21:59.365088       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:21:59.431375       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker"
I0103 23:22:00.466034       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="response-34-worker2"
I0103 23:22:00.466082       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:22:00.579915       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="response-34-worker2"
I0103 23:22:03.682389       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="38.801µs"
I0103 23:22:04.013728       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="182.640804ms"
I0103 23:22:04.013834       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="55.902µs"
I0103 23:22:04.222194       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="207.918949ms"
I0103 23:22:04.222273       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="34.601µs"
I0103 23:22:07.691589       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="16.65976ms"
I0103 23:22:07.691646       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="20.801µs"
==== END logs for container kube-controller-manager of pod kube-system/kube-controller-manager-response-34-control-plane ====
